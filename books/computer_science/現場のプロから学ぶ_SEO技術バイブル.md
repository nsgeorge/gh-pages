---
layout: default
description: 現場のプロから学ぶ　SEO技術バイブル
---

# [現場のプロから学ぶ　SEO技術バイブル](https://www.amazon.co.jp/gp/product/B07FMR3GD6/ref=oh_aui_d_detailpage_o03_?ie=UTF8&psc=1)

---

# SEOの基礎

---

## オンラインでの集客手法

以下のトリプルメディアに分類

 - Owned Media：自社で保有するサイト
 - Paid Media：配信面を購入して出向する他社メディア。リスティング広告、アフィリエイト広告、DSP広告
 - Earned Media：SNSで消費者の評判を得るメディア。Facebook, Twitter, Instaなど

## SEOとは

**検索順位を向上させることで、Owned Mediaからの流入増を目指す施策のこと**

SEOの優位性として、

 - 流入ユーザーの質が高い（自ら検索している＝能動的＝プル型のため）
 - **SEOを施したWebサイトが資産になる**（これ重要）

が挙げられるとのこと。なるほど、SEO施策を施したサイトが資産になるという考え方はなかった。
確かに、リスティング広告はSEO施策のコストをすっ飛ばす代わりに、対価を払って優位な位置に自らのサイトへの誘導動線を表示させていると言えるなぁ。

検索サイトはGoogle+Yahoo! JAPANで９割以上。で、ヤフーの裏側のエンジンはGoogleなので、
９割以上の検索がGoogleのエンジンで行われているという現実。

 - [実はYahooとGoogleの検索エンジンが同じということ知っていましたか！？ - ウェブ企画ラボ](https://webkikaku.co.jp/blog/seo/yahoo-google-search/)

つまり、Googleの検索エンジンに最適化する＝ほぼ世の中の検索エンジンへのSEO対策という話になる。
ので、Googleが公表してるルールにのっとってWebサイト作ろうぜという話

## Googleの評価要素

きた。ここめっちゃ知りたかった。
やっぱり一般には公表されてないらしい。各社ナレッジとかためてそう。
ただ、Googleが過去の公表した内容から推測されるいくつかの要素がまとめられていた。
ざっくり「内部要素」と「外部要素」があるらすぃ

#### 内部要素

 - `<title>`や`<meta description>`の記述が適切か
 - XMLサイトマップを使っているか
 - 構造化マークアップに対応しているか
 - 表示速度は十分か
 - モバイルフレンドリーか
 - コンテンツがユーザーニーズを満たしているか

#### 外部要素

主に他サイトからの引用やリンクがこれに相当するらしい。有名やね。

> Google「Web上の民主主義は機能する」

ただし、意図的に生成した外部リンクは逆にペナルティなどとなるリスクがあるため実施しないようが良い。
あくまで自然にリンクがはられるような有用なコンテンツを作る事が重要（そりゃそうか）


## Search Console

 - [Google Search Console へようこそ](https://search.google.com/search-console/welcome)

ここで、SEO施策が正しく機能しているか、あるいはサイトのパフォーマンスなどをチェックできる。
かなり重要なツール

## HTMLの記述方法

#### title要素

 - ページの内容を端的に表す
   - 検索結果に表示される部分なのでめちゃ重要
 - 31文字以内に収める（検索結果に表示できる文字数がこれくらいなため）
 - Webサイト内すべてのページでユニークなテキストとなるようにする

#### meta要素

 - **文字コードを指定する**
 - **http-equiv="X-UA-Compatible"を指定する**
   - これいらんやろって言ってるひともいるみたい
     - [【HTML】metaタグ：『X-UA-Compatible』はよっぽどでなければ設定する必要はないかもしれません。](https://www.creativevillage.ne.jp/2819)
   - これは、「Internet Explorerでどのバージョンのモードでレンダリングを行うか(ドキュメントモード)を指定することができるプラグマ」らしい
     - [X-UA-Compatibleについて調べてみた - だれも聞いていないと思って歌え](http://38fanjia.hatenablog.com/entry/2016/05/28/164400)
 - **descriptionを記載する**
   - これ検索結果のタイトルの下に表示される部分なのでちゃんと書く
   - ただ、内容によってWebページの記事内容が表示される場合もあり
     - 適切なほうをGoogle検索エンジンが判断して表示してるらしい
   - キーワードを含める（検索結果で太字で表示される場合があるので有効）
   - 100文字以内にする
   - ページごとにユニークな内容にする（これむず）
 - **keywordsを記載する**
   - カンマ区切りで書く
   - 決まりはないが目安は3-6個
   - 関係ないキーワードを設定すると悪用と判断されて評価がさかるかも。注意
   - 検索順位のランキングへは影響ないらしい
 - **robotsを記載する**
   - 後述

#### h要素による見出し

以下の注意点を守る

 - 階層構造を守る
 - h要素を多用しない
   - これGoogleも多用すんなって明言してるらしい
 - `<h1>`の見出しテキストはページごとにユニークにする
   - 1ページに1つだけ
 - スタイルを整えるためだけに使わない

#### a要素によるアンカーテキスト

 - ルートパス：ドメインの最上位を基準にするリンク

がオススメらしい。アンカーテキストは`リンクとなる文言`をちゃんと考えることが重要。そりゃそうか。

 - アンカーテキストが検索順位に影響している事例：「出口」でぐぐる

#### img要素で伝える画像情報

 - alt属性：万が一画像が表示されなかった場合の代替手段として表示させる文言
   - `<img src="http://dog.com/dog1.jpeg" alt="かわいい柴犬の画像"/>`的な感じ
 - title, h, a要素の場合も画像を使うときはalt属性を設定すること
 - あとGoogleBotは画像周辺の文脈も解析してくれるっぽいのでその画像に関連する文章の近くに置くこと
 - あと重要なポイントとして、画像の「ファイル名」もその意味を理解するために使われるらしい
   - 「pic01.jpeg」よりも「`pretty_dog01.jpeg`」が良い

---

# マイナス評価を回避するSEOの話

---

マイナス評価を受けた場合、いくらプラスの施策をうっても効果を発揮できないため、
まずは「マイナス」を「ゼロ」に持っていくところに注力することが重要。

## 1URL1コンテンツの原則

これこの本で何度も出てくる重要な概念。覚えておきたい。

Googleでは、検索結果に可能な限り多様なドメインの検索結果を表示する仕組みが採用されてるらしい。

つまり、同一キーワードに対して同一ドメインで複数ページSEO施策をうっても、
検索結果にはそのうちのどれか１ページしか表示されない可能性が高い。（つまり重複しているページへのSEO施策は無駄になる）


## URLのユニーク化

1URL1コンテンツを守るためにはどうすればよいか？これが守れてないケースの大きなパターンが以下

 - 同一URLに異なるコンテンツ
 - 異なるURLに同一コンテンツ
 - 異なるURLに同一title/description
 - 異なるURLに１コンテンツが分割されている（ページネーション）
 
え、ページネーションもだめなん！？と思った。
なるほど、これはページネーションがだめというよりそのURI設計に気をつけろという意味っぽい。


## 同一URLに異なるコンテンツの解消方法

よくあるのが、

 - http://shop.com/search
 
みたいなURLで検索ページがあって、querystringで検索結果を出し分けてるパターン。
実はこれはOKらしい。問題なのは、クエリがPOSTになってて、
条件を指定して検索してもURLは変わらない、というやつ。これはNG。

可能であれば、「静的URL」が好ましいらしい。

`http://shop.com/search?gender=mens&category=tops&product=t-shirt`

これ（`動的URL`）より

`http://shop.com/mens/tops/t-shirt`

こっち（`静的URL`）が良いという意味。

## 異なるURLに同一コンテンツの解消方法

以下のケースがこれに該当するっぽい

 - http／https
 - wwwの有無
 - index.php／index.html／なし
 - URL末端の「/」の有無
 - 変なquerystringがついているなど
 
これ、何が問題なのか。1URL1コンテンツの原則を満たしてないのもそうやけど、以下の問題が大きいみたい

 - SEO評価が分散してしまう
   - これは、外部リンクを貼ってくれる人が色んなバリエーションのURLでリンクを貼ってしまったりすることもあるため
 - 同じ内容のページを複数回クロールしてしまうためクローラビリティが低くなる
 - クロールバジェットももったいない

で、どうすればよいか

 - http／https：httpsに寄せる 
 - wwwの有無：どっちでもOK
 - index.php／index.html／なし：「/」で終わらせる
 - URL末端の「/」の有無：「/」で終わらせる
 - 変なquerystringがついているなど：パラメタつけない

って感じがおすすめらしい。

また、正規ページを定義してそっちだけクロールしてね！みたいな設定をする必要がある。以下で解説。

#### 異なるURLのページを正規ページへ寄せる方法

以下の２つの方法がある

 - 301リダイレクト
 - canonical設定
 
Googleは「301リダイレクト」を推奨している。

このため、ユーザーに正規URLに遷移させたくない場合にのみcanonical設定を利用するようにする。

ケース別に整理し直すと

 - http／https：301リダイレクト
 - wwwの有無：301リダイレクト
 - index.php／index.html／なし：301リダイレクト
 - URL末端の「/」の有無：301リダイレクト
 - 変なquerystringがついているなど：canonical（※1）
 - 別URL（本当は検索結果上位に表示されているURLが正）：301リダイレクト

という感じ。

※1：アクセス解析等で利用するパラメタがある場合、301リダイレクトを利用すると情報が欠落してしまうため

#### 301リダイレクト

リダイレクトには、以下の3つのやり方があるが

 - サーバー側で設定
 - jsで設定
 - htmlで設定（`meta refresh設定 -> 本に載ってなかったのでggrこと`）
 
301リダイレクトが可能なのは`サーバー側で設定`のみ（Google推奨）。
 
それ以外の方法はリダイレクトはできるもののただの強制的なページ遷移なだけでhttpステータスコードとして301を返したりはできない。
 
サーバーの設定方法は、Webアプリケーションのインフラに依存するので適時調べて設定する。

**jsで設定**

どうしてもサーバー側で制御できない場合のみ、jsとhtmlで制御する方法を採用するようにする。

jsで設定する場合は、リダイレクト元となるページのhtmlに以下のようにスクリプトを埋め込む

```html
<head>
...
  <!-- https://xxxx.comにリダイレクトする場合 -->
  <script type="text/javascript">
    location.href="https://xxxx.com";
  </script>
...
</head>
```

#### canonical設定

これまたやり方が2つある

 - htmlで設定
   - html head内に`<link rel="canonical" href="https://xxx.com">`
 - httpヘッダーで設定
   - http ヘッダーで`Link: <https://xxx.com/yyy.pdf>; rel="canonical"`

どう使い分けるかというと、例えばファイルの表示などでhtmlで指定できない場合にhttpヘッダーの方法を使うらしい。なるほど。

## ページネーション対応

1URL1コンテンツの原則が守られていないパターンに、このページネーションが入っているページも該当するらしい。なぬ

4つのポイントがあるみたい

 - URLの管理方法
 - rel="prev" /rel="next"の設置
 - title/descriptionのユニーク化（これむずくない？）
 - 旧Search Consoleでのパラメタ設定

#### URLの管理方法

ページネーションしているページについて、全て同じURLだけどユーザーに見えないパラメタ（Cookieとか）を使ってページングはNGみたい。

基本的には、以下のどちらかの方法にしないといけない

 - **同一URL階層にパラメタを付与した異なるURL**
   - 例
     - http://xxx.com/zzz?page=1
     - http://xxx.com/zzz?page=2
     - http://xxx.com/zzz?page=3
 - **異なるURL階層で管理**
   - 例
     - http://xxx.com/zzz/1/
     - http://xxx.com/zzz/2/
     - http://xxx.com/zzz/3/
 
 まぁ両方よく見かけるやつ。
 
#### rel="prev" /rel="next"の設置

これ知らなかった。ページの前後関係をhtmlで表現できるらしい。

```html
<head>
...
  <!-- prev: 最初のページ以外に記載 -->
  <link rel="prev" href="前ページURL">
  <!-- next: 最後のページ以外に記載 -->
  <link rel="next" href="次ページURL">
...
</head>
```
    
#### title/descriptionのユニーク化

ほんとにバラバラにするのはむずいので、

 - title: `XXX一覧 Nページ目`
 - description: `XXX一覧のNページ目の紹介ページです。`

という感じでページを含めてあげるのが良いみたい。

ちなみに検索エンジンは1ページ目を検索結果に表示するようなロジックが入っているらしい（2018/08現在）

#### 旧Search Consoleでのパラメタ設定

パラメタでのページネーションの制御の場合は、旧Search Consoleでパラメタ名を登録してあげると良いみたい。

`http://xxx.com/zzz?page=1`の場合は、`page`をパラメタとして登録してあげるということ。

---

# エラーに対する対応の話

---

そもそもエラーを出さないようなサービス運営が重要。

仮にエラーとなってしまう場合でも、ユーザーフレンドリーなページを用意することでGoogleからの評価を落とすことを回避できる。

## エラー対応

エラーは、大きく内部エラーと外部エラーがある

 - 内部エラー：自サイトのエラー
   - 対策：エラー自体を解消する
 - 外部エラー：外部リンクのエラー
   - 対策：リンク切れ等はこちらでは制御できないため、リンクの変更や削除を行う

ちなみに、ユーザーやGooglebotが回遊できないページは対象外になる。

たとえば、サービスの管理者用ページなど。

#### HTTPステータスコード

前提知識としてHTTPステータスコードの概要をまとめる

 - 2xx: サクセス
 - 3xx: リダイレクション
 - 4xx: クライアントエラー
 - 5xx: サーバーエラー
 
#### エラーの探し方

新Search Consoleでエラーを探す機能があるらしい。便利。

#### 404ページをユーザーフレンドリーにする

デフォルトの404ページはユーザーにとって望ましくない事がほとんど。

ユーザーフレンドリーな404ページにするため、以下の項目について検討する。

 - 探しているページがなかったことを伝えられているか？
 - サイトのその他の部分と同じデザインとなっているか？（ナビゲーションなど）
 - 最も人気の記事などへのリンクなど追加できないか？
 - これらの存在しないページがリクエストされた場合にhttpステータス:404を返せているか？
   - これは、Googleの検索結果として意図せずindexingされてしまうことを防ぐため

## スパム対応

スパムとは、Googleのアルゴリズムのすきをつき、ユーザーには不利益を与えるが検索エンジンには高評価を与えられる行為を行うこと。

例えば、以下のようなもの。

 - **価値の低いコンテンツの量産（あるいは自動生成）**
   - コピーコンテンツ、ミラーサイト
   - 自動生成コンテンツ
   - キーワードの詰め込み
   - 広告のみで構成されたコンテンツ
 - **質の低い被リンクの量産（あるいは自動生成）**
   - 低価値なコンテンツからのリンク
   - 無関係なコンテンツからのリンク
   - 購入リンク（金銭によるリンクの購入）
 - **クローラーへの偽装行為**
   - キーワードの詰め込み
   - 隠しテキスト、隠しリンク
   - クローキング（ユーザーとGooglebotに異なるページを見せる行為）

自分が意図していなくても、結果的このような状態になってしまっている場合もありうる。

こういったことがないように、定期的なチェックが必要。

---

# 分かりやすいサイトの構成の話

---

## MECEなカテゴリ設計

サイトの構成は、カテゴリごとに階層を決めるのが良い。

 - ファッション
   - アウター
   - パンツ
   - トップス
     - Tシャツ
     - シャツ
     - ニット

みたいな感じ。

所謂MECEなカテゴリ設計が必要。

> MECEって？ロジカルシンキングの基本的考え方を理解して思考の抜け漏れを防ごう｜ferret ［フェレット］  
> https://ferret-plus.com/4387


## ユーザーの目的にあったカテゴリを設計する

3つのツリーで考える方法が紹介されてた。

 - WAHTツリー
 - WHYツリー
 - HOWツリー
 
#### WHATツリー

カテゴリを軸に分類する方法

 - ファッション
   - アウター
   - パンツ
   - トップス
     - Tシャツ
     - シャツ
     - ニット

#### WHYツリー

頂点に解決したい課題を配置して原因をMECEに分類していく方法

 - コーデが決まらない
   - 自分に合う服が見つからない？
   - 性格の問題？
   - 手持ちの洋服が少ない？
     - お金がない？
     - 買いに行く時間がない？

#### HOWツリー

目標や目的を頂点に配置し手段やプロセスをMECEに分類していく方法

 - 配達してほしい
   - 時間指定したい
   - 配達方法を指定したい
   - 梱包方法を提案したい
     - ギフトラッピング
     - 簡易包装

## URLの命名規則

ユーザーがURLを見ただけでどういうページがそのコンテンツ内容が容易に想起できることが重要みたい。

で、ここでユーザーとURLがどういう場所で接点を持つかについて述べられていた。

 - 検索結果ページ
 - SNSでのシェア
 - アドレスバー
 
が大きくあるらしい。加えて、Webページ上のリンクとかもあるかなと思った。

#### クローリング、検索結果への影響

実は、現在のアルゴリズムだとURLの規則によって検索結果が上位に来たり（過去にはあったらしい）、クローリング効率がUPしたりすることはないみたい。

ただ、ユーザーや検索エンジンに有益なことは間違いないので抑えておいたほうが良いとのこと。


#### 分かりやすいURLとは

ここは結構具体的な話が出てて参考になった。URLの命名規則について具体例を使って説明されていた。

 - **id等の識別子は使わない**
   - NG: http://xxx.com/15/24/
   - OK: http://xxx.com/tops/tshirt/
 - **できるだけ短くする**
   - NG: http://xxx.com/clothes-tops/t_shaped_shirt/
   - OK: http://xxx.com/tops/tshirt/
 - **長くなる場合は`-`で適切に区切る**
   - NG: http://xxx.com/tops/redspringcoat/
   - OK: http://xxx.com/tops/red-spring-coat/

区切り文字は、`-`を使うべきとのこと。Googleもハイフンで文字を区切ることを推奨しているみたい。

あと、文字はすべて小文字で統一したほうが良い。大文字と小文字が混ざるとURLの入力ミスなどに繋がるため。

#### URLの長さ

原則短いほうが良いみたい。理由は以下。

 - やたら長いURLは見栄えが良くない（不安を煽る）
 - URLには2048文字の上限が設けられているため

## 辿りやすいサイト内部リンク

3つポイントがあるらしい。

 - すべてのページを双方向のリンクで繋げる
 - 関連するページ（カテゴリ）にリンクをはる
 - 常にトップページ、最上位カテゴリへのリンクを貼る（よく見かけるやつ）

関連するページのリンクを正しくはることで、検索エンジンにもページ同士の関連性を伝えることができる。

→　各コンテンツの意味をより正確に検索エンジンに伝えられる

また、トップページへのリンクを全ページに設定することで、
Googlebot（そしてユーザーも）が最上位カテゴリへのリンクをハブにして少ない遷移数でサイト全体へ遷移できる。

この結果、クローラビリティが向上するので良いらしい。

また、表示しているページのメインコンテンツからのリンクはページの内容との関連が高いと判断されるため、

ナビゲーションのメニューからのリンクなどと比べて、SEO効果が最も高い。

加えて、内部リンクの表記ゆれがないように注意する

 - href="/xxx/yyy.html"
 - href="/yyy.html"
 - href="/yyy"
 - href="/yyy/"

上が同じページを指しているとすればこれは表記ゆれに該当する。

#### ナビゲーションを設置する

ナビゲーションとは、サイトのトップやサイド、ボトムにあるメニューのこと。

大きく分けて3種類ある。

 - **グローバルナビゲーション**
   - Webサイト全体で共通で設置されているナビゲーション
   - 常にトップページ、最上位カテゴリへのリンクを張る役割
 - **サイドナビゲーション**
   - ローカルナビゲーションともいう
   - 主にサイトの左側、あるいは右側に配置されるナビゲーション
   - 今見ているカテゴリの他のカテゴリへの導線を設置する役割
   - おすすめページやランキングを設置する場合も
 - **フッターナビゲーション**
   - サイトの下部に設置されるナビゲーション
   - 利用規約や個人情報保護方針などの補足的な情報を扱う
   
#### パンくずリストを設置する

トップページから現在表示しているページまでの階層を示すリンク集。

アマゾンとか楽天とかで見かける、

 - `パソコン用品 > デスクトップ > モニタ > 20インチ`
 
みたいなやつ。

#### htmlサイトマップ

WebサイトのリンクやURLを地図のようにまとめたページのこと。

2種類あり

 - **htmlサイトマップ**
   - ユーザビリティ向上
   - ユーザーが見れる
 - **xmlサイトマップ**
   - Googlebotのクロール専用

となっている。で、現在はxmlサイトマップのみで十分らしい。

#### 避けるべきリンク形式

一応jsでの動的に生成されるリンクについてもgooglebotは問題なくクローリングしてくれる。

ただし、静的ファイルの解析よりも時間を要する場合があり、すぐクローリングしてほしいなら静的ファイルであるhtmlに記載することが望ましいそう。

できれば避けたいリンク形式は以下

 - jsによるリンク（生成されたものなど）
 - Adobe Flashによるリンク
 - プルダウンメニューによるリンク
 - 検索フォームによるリンク

## カテゴリ階層とURL階層が異なるケース

1つのコンテンツを複数カテゴリ配下にぶら下げるパターンもよく見られるそう。

例えば、賃貸サイトなどがそれ。

 - トップ > 東京都 > 新宿区 > X丁目 > 物件A
 - トップ > 中央線 > 新宿駅 > 西口 > 物件A

みたいなノリ。

---

# Googlebotを制御する話

---

## クロールの制御

googlebotのクロールの挙動を制御する方法がある。

以下の3つ

 - [a] htmlの`meta robots`で指定（htmlファイル用）
 - [b] httpヘッダーの`X-Robots-Tag`で指定（非htmlファイル用）
 - [c] robots.txtで指定

[a], [b]は設定できる項目は同じ。

何を制御するの？については、クロールさせなくて良いページの指定、インデックスさせたくなページの指定など。

なんでこんな制御するのかというと、実はgooglebotのクローリングは無制限に行われるわけではなく、

サイトに応じたクロールバジェット（クロール量）が割り当てられていて、不要なページをクロールの対象外にすることで
クロールの効率をUPできるらしい！！

#### htmlの`meta robots`で制御

以下の様に指定する

```html
<head>
...
  <meta name="robots" content="noindex, nofollow">
...
</head>
```

設定できるcontent属性は以下

> Google がサポートしているメタタグ - Search Console ヘルプ  
> https://support.google.com/webmasters/answer/79812?hl=ja

なお、index, followについてはデフォルトの挙動となっているのでmetaタグで指定する必要はない。

#### X-Robots-Tagで指定

こちらもPDFファイルなどの非HTMLの場合にHTTPヘッダーで指定するもの。

```
X-Robots-Tag: "noindex"
```

という感じで各Webサーバー側のアプリケーションで設定する。

#### 各設定の利用シーン

 - **noindex：インデックスさせたくないときに利用する**
   - 検索結果が0件のページ
   - 質が低いページ
   - キャンペーンページなどの検索結果から流入させたくないページ
   - 会員ページ、管理者ページ
 - **nofollow：辿らせたくないリンクに利用する**
   - 辿らせたくないページへのリンクに設定する
   - a要素に`rel="nofollow""`で設定する
 - **noarchive：▼を押したときのキャッシュへのリンクを無効化する**
   - 通販サイトやニュースサイトなどの時期によって変化するページ
 - **unavailable_after：指定した日時をすぎると検索結果からページを削除させる**
   - イベントや期間限定で検索結果に載せたい場合
 

#### robots.txtについて

このファイルはクロールの禁止を設定するものだが、禁止しない場合でも常に配置しておくのが良いらしい。

この場合は、「どのページもアクセス可能」という設定を配置しておく形になる。

robots.txtは

 - ユーザーエージェント
 - アクセス可否
 - 対象ディレクトリ
 
等を記述して、クローラーの挙動を制御することができる。詳細は割愛。


## クロールを促進させる

クロールを促進させる方法は以下の4つあるらしい。

 - XMLサイトマップ（全ページ対象 x クロールを待つ）
 - Fetch as Google（全ページ対象 x クロールを要求する）
 - RSS/Atomフィード（新着ページ対象 x クロールを待つ）
 - WebSub(PubsubHubbub)（新着ページ対象 x クロールを要求する）


### XMLサイトマップ

 - サイトが新しく、外部リンクがない
 - ページ数が非常に多い
 - （本来望ましくないが）サイトにどこからもリンクされてないページがある
 
場合に特に威力を発揮するみたい。ただ、これらに該当しなくても基本は作成しておくべきとのこと。

設定方法は割愛するとして、作成したあとは

 - robots.txtみたいに所定の場所に配置する（場所は設定で変更可能）
 - 新Search ConsoleでサイトマップURLを登録する
 
のステップが必要

また、XMLサイトマップファイル一覧をまとめたXMLファイルを`サイトマップインデックス`と呼ぶらしい。

#### XMLサイトマップによる画像／動画のインデックス促進

htmlのimg要素を使うだけでなく、xmlサイトマップを使って画像の存在と意味を伝えることができる。

これを`画像サイトマップ`と呼ぶみたいで、これを使うとより効率的に画像をインデックスに登録してもらえるみたい。

XMLサイトマップに記載してもよいし、画像用に別のXMLサイトマップを作成しても良い。

また、画像だけでなく動画も同じようにサイトマップに記載することでインデックスへの登録を促進できる。


### Fetch as Google

旧Search Consoleで実行できる、クロールの要求機能のこと。

いつまでもインデックスされないページなどはこの機能を使って能動的にクロールを促進させることができる。


### RSS/Atomフィード

フィードを用意しておくことで、更新情報をクローラーに効率よく伝えることができる。

GoogleもXMLサイトマップとフィードの両方を用意しておくことを推奨しているらしい。

フィードの仕様について本を参照。

`.xml(.atom)`or`.rss`で保存し、特に意図がなければドメイン直下に配置する。

```
https://example.com/feed.rss
```

#### フィードの場所を検索エンジンに伝える方法

以下の方法でフィードの場所を検索エンジンに伝えることができる。

 - 新Search Consoleで登録する
 - htmlのhead要素内にlinkタグ`type=application/rss+xml`で明記する

フィードは基本トップページのhead要素内に書けば良いが、大規模なサイトの場合全体の新着情報はユーザーにとって不要な場合が多い。

こういう場合は、カテゴリの階層ごとにフィードを用意しておいてそれぞれフィードを配る方法があるみたい。

特に、Fetch as googleやと手動でのリクエストが大変な場合とかは、自分でWebSubシステムを実装して、

クロールリクエストを自動化できるらしい。

### WebSub (PubSubHubub)

Googlebotやフィードリーダーにプッシュ通知によって新着ページを連絡する仕組みのこと。

出版社（Publisher）と購読者（Subscriber）をハブ（Hub）を通じて繋げる仕組みを指す。

 - 出版社：サイト運絵者
 - 購読者：Googlebot, フィードリーダー
 - ハブ：Hubサーバー
 
このとき、Hubサーバーはいくつか世の中に存在しているみたい。

で、検索エンジンへのプッシュ通知（Googlebotへの通知）の場合は、Googleが用意しているHubサーバーを利用する、で問題ないっぽい。

> Google Pubsubhubbub Hub  
> http://pubsubhubbub.appspot.com/


#### WebSubの利用方法

 - フィードにHubのURLを記述する
 - GoogleのHubサーバーにrssフィードのURLをpostする
 
また、

> Google Pubsubhubbub Hub - Publisher debug  
> http://pubsubhubbub.appspot.com/publish

にアクセスして手動でプッシュすることも可能っぽい。


---

# セマンティックなマークアップ

---

htmlの構造に加えて、その意味を明示できる記述方法がhtml5には用意されている。大きく以下の2種類。

 - 構造化タグ：文書構造を明示する（html5のみ）
 - 構造化マークアップ：会社情報や商品情報の意味を明示する

## 構造化タグ

 - section : セクション ※これが基本
 - article : 記事
 - aside : 補足情報
 - nav : ナビゲーション
 - header : ヘッダー
 - footer : フッター

それぞれの意味の詳細は本を参照。あるいはググる。

上のすべての基本となるのが「section」で、
これは見出しをつけられる文章のまとまりを意味する。
また、section, article, aside, navは「セクション」を区分するタグと呼ばれ、
これらのタグのコンテンツを「セクショニングコンテンツ」と呼ぶらしい。

※ sectionタグとセクションは別の概念。「セクションを区切るタグ」の中にsectionタグがあるイメージ。

### 間違いやすい誤用

以下のパターンは誤用なので注意する

 - 独立セクションにも`article`ではなく`section`を利用してしまう
   - articleタグはsectionタグに「コンテンツの独立性」が付与されたものなので独立して使って良い
   - 1つの記事を意味のまとまりごとに段落分けする場合
     - `<article><section></section></article>`
   - 意味のまとまった複数の記事の場合
     - `<section><article></article></section>`
 - `<div>`を構造化タグに置換してしまう
   - divには文書構造や意味を明示する意味は一切ないので不適切
   - セクションとしてのhtml構造を再考する必要がある
 - `<section>`内のh要素をすべて`<h1>`から開始してしまう
   - これはまぁ・・・こだわりの範囲かも（意見分かれてる）
     - W3C : 1ページにh1は一つだけにしてね
     - WHATWG : 1ページに複数のh1置いてもOK
   - ぶっちゃけh1から毎回開始しても検索には影響ないみたい
   - ただ人の見た目的に文書構造が分かりにくくなるので注意


## 構造化マークアップ

要素に意味を付与できる。    

意味を付与することで、検索エンジンがかしこくリッチな検索結果を返せるようになる（＝ユーザーに適切な情報を提示できる）。

例えば、`フランス`は国、`フランスパン`は食べ物、みたいなことを考慮した検索結果を返せるようになる。

構造化マークアップは、検索結果にダイレクトに結果が反映される。例えば以下。

 - 検索結果の拡張表示
   - レストランならその評価が★★★★で表示されたり、価格帯が表示されたりするやつ
 - ナレッジパネルの表示
   - ブラウザの右側やスマホの上部に表示される情報

**企業情報の例**

<img src="https://i.imgur.com/lkPIOeP.png" width="60%" height="60%" style="border:0;box-shadow:0 0 0 0" alt="Semantic_Markup_sample_organization">

**商品・口コミ情報の例**

<img src="https://i.imgur.com/FuB1ZBV.png" width="60%" height="60%" style="border:0;box-shadow:0 0 0 0" alt="Semantic_Markup_sample_review">



### 構造化マークアップの仕様

以下の3形式ある

 - **Microdata**
   - html内で独自のタグによりマークアップする（`itemprop="name"`とか）
   - 多くのWebサイトに普及している
 - **RDFa Lite**
   - html内に記述する点はmicrodataと同じ
   - xhtmlを使って名前空間を拡張できるのが特徴
   - でもhtml5が普及してきているのでこれを積極的に使う理由はない
 - **JSON-LD**
   - html内のscrpit要素内に、jsonで記述する
   - Googleが推奨している形式なのでこれを使うと良いかも
   - htmlを汚さないことと、人が読みやすいというメリットがある
   - 逆にhtmlとjsonを両方メンテしないと行けないデメリットもあり

### 構造化マークアップで検索エンジンに伝えられるもの

以下のような情報は、構造化マークアップによって検索エンジンに詳細情報を伝えリッチに検索結果に表示させることができる。

 - **組織情報**
   - 住所とか電話番号とかロゴとか
 - **パンくずリスト**
   - 検索結果にURLの代わりにパンくずリストが表示されてユーザーが見やすい
 - **商品・口コミ情報** 
   - 商品の評価や価格など
 - **サイト内検索**
   - スニペットに自サイトの検索フォームが表示される

これらの構造化マークアップは

> SearchAction - schema.org  
> https://schema.org/SearchAction

にまとめられていて他にもたくさん種類がある。詳細は本の表が分かりやすい。

### 構造化マークアップ支援ツール

> Structured Data Markup Helper  
> https://www.google.com/webmasters/markup-helper/u/0/?hl=en

これでJSON-LD形式のマークアップを作成できる。

また、作成したマークアップは

> 構造化データ テストツール  
> https://search.google.com/structured-data/testing-tool/u/0/?hl=ja

でテストできる。

さらに、旧Search Console「構造化データ」画面でエラーが出てないか確認することもできる。


---

# サイトの高速化

---

読み込みに5秒かかると90%のユーザーが離脱するという報告があるらしい

> Bottom Line Loading Time - Blog
> https://blog.kissmetrics.com/wp-content/uploads/2011/04/loading-time.pdf

その結果、表示が1秒遅れるごとに顧客満足度は16%落ち、44%が友人にオンライン上で悪い体験を共有するそう。

## 高速化に寄与する処理

以下の3つがある

 - レンダリング処理
 - ネットワーク処理
 - サーバー処理

### レンダリング処理を高速化する方法

レンダリングの流れは以下

 1. HTML解析
 1. DOMツリー構築
   1. JS解析／実行
 1. CSSCMツリー構築
   1. JS解析／実行
 1. レンダリングツリー構築
 1. レイアウト
 1. ペインティング

#### レンダリング処理の高速化（CSS編）

CSSでできることは以下の3つ

 - **CSSファイルをWebブラウザに早期に読み込ませる**
   - head要素の先頭に記述すればOK
 - **ファーストビューで利用しないCSSをレンダリング前に読み込ませない**
   - ファーストビューで利用するCSSはhead要素で
   - それ以外のCSSをbody要素で読み込ませる
   - ただし、この切り分けができない場合は無理に対応しなくて良い
 - **CSSの解析スピードを上げる**
   - CSSセレクタは右から左へ解析される
   - 具体的かつ少数のセレクタで指定すると処理が速い
   - これらを抑えておくと良いって書いてあった


#### レンダリング処理の高速化（js編）

jsファイルの読み込みでできることは以下の2つ

 - **HTMLのbody要素の最後に記述する**
   - headで記述するとjsの解析が終わるまでbodyが読み込まれないため
   - ただし、Google analyticsタグなど確実に読み込ませたいものはheadで記述する
     - bodyが読み込まれる前にユーザーが離脱してしまうかもしれないため
 - **非同期に読み込ませる**
   - scriptタグに`async`属性か、`defer`属性を指定する
   - async
     - DOMツリー構築前後に関わらず実行
     - 複数の場合はファイルの読み込みが終わった順に実行
   - defer
     - DOMツリー構築完了後に実行
     - 複数の場合はHTML記述順に従って実行 


#### レンダリング処理の高速化（画像ファイル編）

画像ファイルダウンロード前に画像サイズを指定する（CSSなどで）

つまり、width, heightを指定する。


 
### ネットワーク処理を高速化する方法

ネットワーク処理の時間は以下の掛け算で決まる

 - 通信量
 - 通信回数
 - 通信距離
 
#### 通信量を減らす

Webでの通信では、一般的にテキストファイルと画像ファイルが大半で、それぞれ

 - テキストファイル：miniy圧縮
   - ※テキストファイルは、html, js, cssすべて含む
 - 画像ファイル：gzip圧縮 
   - さらに、画像形式自体を圧縮率の高いフォーマットに変更するのもあり

で通信量を削減できる。

画像フォーマットの種類と特徴については書籍参照。

#### 通信回数を減らす（新プロトコルの利用）

HTTP/2を使うと通信回数を減らせる。

 - **HTTP/1.1**
   - 1コネクション1通信
   - ヘッダー圧縮不可
 - **HTTP/2**
   - 1コネクション複数通院
   - ヘッダー圧縮可

HTTP2を使うには、サーバー側でソフトウェアの導入、設定が必要（Apache等）。

ちなみに、2018/09時点ではGooglebotはHTTP/2に対応してないが将来的に対応する可能性は高そう。

#### 通信回数を減らす（bundle処理でファイル数を削減）

ファイル通信に関する通信回数の削減方法は、大きく以下の3つの対応方法がある。

 - **複数ファイルを1つにまとめる（Bundle処理）**
   - js, cssなど、複数ファイルを1つのファイルにまとめる
   - CSSスプラウトという手法で、画像をまとめることも可能
   - 意味合いを強く持たせたい画像は単独の画像として利用したほうが良い
 - **ページ個別のjs, cssファイルをインライン化する**
   - 共通のjs, cssはキャッシュさせるためにファイルで読み込むのがベター
   - ページ個別のjs, cssはそのためだけの通信を防ぐためにインライン化すると良いみたい
 - **画像の遅延読み込みを利用する（Lazy Load）**
   - 画像がページ内に入ったタイミングでロードさせる手法
   - 一気に全体の画像が読み込まれないので初回の通信回数を削減できる
   - jQueryが必要らしい。むむ。

#### 通信距離を減らす

これはCDNを利用して地理的に近いサーバーにファイルをキャッシュさせるのが有効。

有名なCDNをいくつか

> Cloudflare - Webパフォーマンスとセキュリティを追求する企業 ｜ Cloudflare  
> https://www.cloudflare.com/ja-jp/

> 無料CDNでWEB高速化 ｜ Rapid START  
> https://cdn.tokyo/

> CDN サービス｜クラウドコンピューティングサービス｜ Akamai JP  
> https://www.akamai.com/jp/ja/

> Edge Cloud Platform ｜ Fastly  
> https://www.fastly.com/


### キャッシュ活用などの高速化手法

#### Resources Hintsを利用する

`Resources Hints`と呼ばれる方法があり、以下の4つの機能がある。

 - **dns-prefetch（DNSの事前ルックアップ）**
   - HTMLの読み込みと同時に外部ドメインをバックグラウンドでDNS読み込みする
   - `<link rel="dns-prefetch" href="xxx">`をheadに記述する
   - 上で記述されたドメインはバックグラウンドでIPが取得される
   - 以降、DNSを介さずに直接IPで通信する
   - 特に外部ドメインのリソースが多いときに有効
   - デメリットがほぼないので積極的に活用したい
 - **preconnect（TCPの事前接続）**
   - HTMLの読み込みと同時に外部ドメインとのTCPコネクションをバックグラウンドで確立する
   - `<link rel="preconnect" href="xxx">`をheadに記述する
 - **prefetch（リソースの事前DL）**
   - 画像やjsなどを読み込み前に事前取得させられる
   - `<link rel="prefetch" href="xxx.jpeg" as="image">`をheadに記述する
 - **prerender（次ページの事前レンダリング）**
   - 次ページのリソースを取得してレンダリングまで実行する
   - `<link rel="prerender" href="/next.html">`をheadに記述する
   - 1ページに指定できる`prerender`は1ページのみ

#### キャッシュを利用する

キャッシュできる箇所は以下の2箇所

 - **ブラウザキャッシュ**
   - `.htaccess`でファイル形式ごとにキャッシュ保持期間を設定する
 - **サーバーキャッシュ**
   - サーバー側でのキャッシュ実装で対応
   - DBのデータとか外部リソース類など
 
#### PageSpeed Moduleの導入

Googleが提供している`PageSpeed Module`を使うと簡単にまとめて高速化できるらしい。

Apache, Nginx向けが提供されている。

> PageSpeed Module  ｜  Google Developers  
> https://developers.google.com/speed/pagespeed/module/?hl=JA

### 表示速度の測定ツール

 - Lighthouse
 - PageSpeed Insights

### サーバー処理の高速化

リソースサーバーとロジックを持つサーバーを分離したり、台数増やしたりスペックアップしたりなどなど


---

# HTTPS化／モバイル対応／AMP対応

---

## HTTPS化

https化することで安全な通信が行える上に検索のランキング的にもちょっと優遇される。

具体的な対応方法はググるとして、https化したあとの作業として以下がある。

 - httpからhttpsへのリダイレクト
 - html内のリンクをhttpsに書き換え
 - 新Search ConsoleにhttpsのURLを登録する
 - 証明書期の限切れ前に更新する
 

## モバイル対応

 - **モバイルフレンドリーアップデート**
   - モバイル端末で読みやすいサイトの順位を上げるGoogleのアップデート（2015/4/21〜）
     - タップやズームがなくても読みやすい
     - タップ領域の間隔が適切
     - 再生できないコンテンツが含まれていない
     - 横方向へのスクロールが発生しない
 - **モバイルファーストインデックス**
   - 評価対象をPCページからモバイル用ページに軸を移動させたアップデート（2018/03〜）
     - モバイルサイトを評価軸の基準とし、PCサイトをセカンダリとして扱う
     - PC版サイトのみの場合はPC版をインデックスとして評価する

### モバイル対応ページの作成

以下の2つの方法がおすすめ

 - **レスポンシブデザイン**
   - 同一URL, 同一HTML
   - メディアクエリを使って画面のレイアウトを制御
   - Googleはレスポンシブデザインを推奨している。
   - クロールバジェットを節約できる（PC用、モバイル用と別々にクロールせずにすむ）
 - **ダイナミックサービング（動的な配信）**
   - ユーザーエージェントに基づくデバイス検出で配信するHTMLを切り替える

### モバイルフレンドリーなUI

以下のルールは最低限守ったほうがよいらしい。

 - 読みやすいフォントサイズ・行間
 - タップ要素間に適切な間隔を空ける
 - 再生できないコンテンツを掲載しない
 - インタースティシャルを回避する（ページのコンテンツを覆ってしまうようなコンテンツの回避）
 - 表示速度を早くする（前の項でもあった）

具体的な基準は書籍を参照。このあたりは最低限守りたい。

以下のテストツールで検証可能らしい。

 - モバイルフレンドリーテストツール
 - 新Search Console
 - Chrome DevTools


## AMP対応

 - Accerelated Mobile Pages（雷マークみたいなやつ）
 - モバイルページを高速表示させるための技術
 - GoogleとTwitterが共同で開発した仕様でOSS
 - 独自の`AMP HTML`を使う
   - JS利用や外部CSSの読み込み禁止
   - GoogleのAMP CacheというCDNを使い高速表示される

### 導入する意義

 - 検索結果画面でリッチな表示（＝クリック率向上）
 - 高速表示によるユーザー体験向上
 - トップニュースエリアへの表示
 - AMPカルーセルへの表示

[Imgur](https://i.imgur.com/ZC0e0ur.png)

**トップニュースエリアの例**

<img src="https://i.imgur.com/9OjLyoE.png" width="60%" height="60%" style="border:0;box-shadow:0 0 0 0" alt="Amp_sample_top_news">

**AMPカルーセルの例**

<img src="https://i.imgur.com/tiBxrhk.png" width="60%" height="60%" style="border:0;box-shadow:0 0 0 0" alt="Amp_sample_amp_carousel">

### 実装方法

`AMP HTML`仕様を守ったHTMLを作る

 - CSSはインライン記述（orインクルード展開）
 - img要素に縦横のサイズを記述する
 - 独自のJSの読み込み禁止
   - ただし多くのAMP用コンポーネントが用意されている（AMP JS）
 
詳細の実装はググるなり本をみるなり。


---

# コンテンツSEO

---

Googleに評価されるコンテンツとはなにかについてまとめられていた。

以下の評価軸があるらしい（昔はキーワードとの関連性が重要視されていた）。

 - Page Quality評価（ページの品質）
 - Needs Met評価（ユーザーニーズとの一致）

## Page Quality評価

 - **専門知識・権威・信頼性**
   - E, A, T (Expertise, Authoritativeness, Trustworthiness)
   - その分野に対して知見があれば専門知識を有していると判断される
   - その分野のコンテンツによく登場するサイトは権威を持つと判断される
   - 騙すような内容や信頼されない企業などの場合は信頼性がないと判断される
   - これらを継続的に発信することで時間経過とともに権威性や信頼性が高まる
 - **メインコンテンツの質と量**
   - メインコンテンツがページの目的を達成するための品質と量を備えているかどうかの指標
   - 目的とは、例えば「通販サイト」なら「製品に関する情報提供や販売」という具合
   - 例：通販師となら、ユーザーが購入を決意できる情報が分かりやすく記載されていること
   - 量はいたずらに量産すると低い評価を受けてしまうので注意
 - **Webサイトの情報と責任の所在**
   - 問い合わせ先やサービス情報、責任者などを掲載することで信頼できると評価される
 - **Webサイトの評判**
   - 外部からのリンクやサイテーションが評価される
   - サイテーション：テキストでの言及のこと
 - **YMYL関連のページは通常よりも高品質なコンテンツが求められる**
   - YMYL: Your Money Your Life
   - 幸福、健康、金銭などに直結するページのこと
   - 「日本語検索アップデート（2017/02/03）」で、根拠のない医療情報の順位が大きく下落した
     - このアップデートで以下のようなサイトの順位も下落した
       - 質の低いコンテンツを量産しているサイト
       - 画像が多くテキストコンテンツの少ないサイト
       - 内部リンクでページが移動できないサイト

> Google ウェブマスター向け公式ブログ： 良質なサイトを作るためのアドバイス  
> https://webmaster-ja.googleblog.com/2012/09/more-guidance-on-building-high-quality.html

## Needs Met評価

